{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install visionts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "# from eval_gluonts import dataset\n",
    "\n",
    "# run `pip install visionts` in advance to install the package\n",
    "from visionts import VisionTS, freq_to_seasonality_list, VisionTSpp\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VisionTS++ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace repo_id\n",
    "repo_id = \"Lefei/VisionTSpp\"\n",
    "\n",
    "# local directory to save the VisionTS++ model, you can change it to any directory you want\n",
    "local_dir = \"./hf_models/VisionTSpp\"\n",
    "\n",
    "# download the model\n",
    "# If network is slow, you can try download directly from the huggingface repository!\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    local_dir=local_dir,\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You should choose either \"base\" or \"large\" for model size of VisionTS++ !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! IMPORTANT: You should choose either \"base\" or \"large\" for model size !!!\n",
    "\n",
    "MODEL_SIZE = \"base\"\n",
    "# MODEL_SIZE = \"large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_SIZE == \"base\":\n",
    "    ARCH = 'mae_base'\n",
    "    ckpt_path = os.path.join(local_dir, \"visiontspp_base.ckpt\")\n",
    "else:\n",
    "    ARCH = 'mae_large'\n",
    "    ckpt_path = os.path.join(local_dir, \"visiontspp_large.ckpt\")\n",
    "\n",
    "if not os.path.exists(ckpt_path):\n",
    "    raise FileNotFoundError(f\"Checkpoint file not found: {ckpt_path}\")\n",
    "\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "# ===== Load the model =====\n",
    "model = VisionTSpp(\n",
    "    ARCH,\n",
    "    ckpt_path=ckpt_path,\n",
    "    quantile=True,\n",
    "    clip_input=True,\n",
    "    complete_no_clip=False,\n",
    "    color=True\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Model loaded from {ckpt_path}, with model size: {MODEL_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "FIGURES_DIR = './figures_demo/'\n",
    "\n",
    "\n",
    "def show_image(image, title='', cur_nvars=1, cur_color_list=None):\n",
    "    # image is [H, W, 3]\n",
    "    \n",
    "    # handle the color channels of the image according to the color_list\n",
    "    cur_image = torch.zeros_like(image)\n",
    "    cur_image = cur_image.cpu()\n",
    "    \n",
    "    height_per_var = image.shape[0] // cur_nvars\n",
    "    for i in range(cur_nvars):\n",
    "        cur_color = cur_color_list[i]\n",
    "        cur_image[i*height_per_var:(i+1)*height_per_var, :, cur_color] = \\\n",
    "            (image[i*height_per_var:(i+1)*height_per_var, :, cur_color].cpu() * imagenet_std[cur_color] + imagenet_mean[cur_color]) * 255\n",
    "    cur_image = torch.clip(cur_image, 0, 255).int()\n",
    "    \n",
    "    plt.imshow(cur_image)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "FONT_LEGEND = 8\n",
    "def run(x, y, periodicity, nvars=1, color_list=None):\n",
    "    # input shape: [n_batch, context_len, n_vars]\n",
    "    # output shape: [n_batch, pred_len, n_vars]\n",
    "    x = torch.Tensor(einops.rearrange(x, 't m -> 1 t m')).to(DEVICE)\n",
    "    y = torch.Tensor(einops.rearrange(y, 't m -> 1 t m')).to(DEVICE)\n",
    "    \n",
    "    # Before calling forward, make sure you use update_config() to update hyperparameters, context length or prediction length.\n",
    "    model.update_config(context_len=x.shape[1], pred_len=y.shape[1], periodicity=periodicity, \n",
    "                        num_patch_input=7, padding_mode='constant')\n",
    "    \n",
    "    # Forecasting time series\n",
    "    with torch.no_grad():\n",
    "        y_pred, input_image, reconstructed_image, nvars_output, color_list_output = model.forward(x, export_image=True, color_list=color_list)\n",
    "\n",
    "    # model returns a list, which contains the median predicted values and the corresponding quantile predictions\n",
    "    y_pred, y_pred_quantile_list = y_pred\n",
    "\n",
    "    \n",
    "    # Visualization\n",
    "    plt.subplot(2, 2, 1)\n",
    "    show_image(input_image[0, 0], f'input {nvars} vars', nvars, color_list)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(x.cpu()[0, :, 0])\n",
    "    plt.plot(torch.arange(y.shape[1]) + x.shape[1], y.cpu()[0, :, 0], label='true', alpha=0.5)\n",
    "    plt.title('input time series 1st variate')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    show_image(reconstructed_image[0, 0], 'reconstructed', nvars, color_list)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(x.cpu()[0, :, 0])\n",
    "    plt.plot(torch.arange(y.shape[1]) + x.shape[1], y.cpu()[0, :, 0], label='true', alpha=0.5)\n",
    "    plt.plot(torch.arange(y.shape[1]) + x.shape[1], y_pred.cpu()[0, :, 0], label='pred')\n",
    "    \n",
    "    if y_pred_quantile_list is not None:\n",
    "        for j, y_pred_quantile in enumerate(y_pred_quantile_list):\n",
    "            if j < len(y_pred_quantile_list) // 2:\n",
    "                plt.plot(torch.arange(y.shape[1]) + x.shape[1], y_pred_quantile.cpu()[0, :, 0], label=f'pred_0.{j+1}', color=f'C{j+2}', alpha=0.5)\n",
    "            else:\n",
    "                plt.plot(torch.arange(y.shape[1]) + x.shape[1], y_pred_quantile.cpu()[0, :, 0], label=f'pred_0.{j+2}', color=f'C{j+2}', alpha=0.5)\n",
    "    \n",
    "    plt.title(f'ctx={x.shape[1]}, pred={y.shape[1]}, period={periodicity}')\n",
    "    plt.legend(fontsize=FONT_LEGEND)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    FIG_WIDTH = 17.5\n",
    "    FIG_HEIGHT_PER_VAR = 1.5\n",
    "    FONT_S = 13\n",
    "    FONT_L = 15\n",
    "    IM_ALPHA = 0.8\n",
    "    def visual_ts(true, preds=None, name='prediction.pdf', lookback_len_visual=300, pred_len=100):\n",
    "        nvars = true.shape[1]\n",
    "        \n",
    "        # change to numpy array\n",
    "        if not isinstance(true, np.ndarray):\n",
    "            true = true.cpu().numpy()\n",
    "        if preds is not None and not isinstance(preds, np.ndarray):\n",
    "            preds = preds.cpu().numpy()\n",
    "        \n",
    "        # Create a figure with `nvars=7` rows and 1 column, and set the height ratio of each row to 1.\n",
    "        fig, axes = plt.subplots(nrows=nvars, ncols=1, figsize=(FIG_WIDTH, nvars * FIG_HEIGHT_PER_VAR), sharex=True,\n",
    "                                    gridspec_kw={'height_ratios': [1] * nvars})\n",
    "        \n",
    "        # remove the vertical space between subplots\n",
    "        plt.subplots_adjust(hspace=0)\n",
    "        \n",
    "        # cut the lookback-window for visualization\n",
    "        true = true[-lookback_len_visual - pred_len:]\n",
    "        if preds is not None:\n",
    "            preds = preds[-lookback_len_visual - pred_len:]\n",
    "        \n",
    "        # iterate each variable\n",
    "        for i, ax in enumerate(axes):\n",
    "            # ground-truth values\n",
    "            ax.plot(true[:, i], label='GroundTruth', color='gray', linewidth=1.5)\n",
    "            \n",
    "            # prediction values\n",
    "            if preds is not None:\n",
    "                # only visualize the prediction within the lookback-window\n",
    "                ax.plot(np.arange(lookback_len_visual, len(true)), preds[lookback_len_visual:, i], label='Prediction_0.5', color='blue', linewidth=1.5)\n",
    "            \n",
    "            # add a vertical line to separate the lookback-window and prediction\n",
    "            y_min, y_max = ax.get_ylim()\n",
    "            ax.vlines(x=lookback_len_visual, \n",
    "                    ymin=y_min, \n",
    "                    ymax=y_max, \n",
    "                    linewidth=1, \n",
    "                    linestyles='dashed', \n",
    "                    colors='gray')\n",
    "            \n",
    "            # remove the x-axis and y-axis ticks and labels\n",
    "            ax.tick_params(axis='x', which='both', length=0)\n",
    "            ax.tick_params(axis='y', which='both', length=0)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # add a text label to the top-left corner of the subplot\n",
    "            ax.text(0.007, 0.8, f'Var {i+1}', transform=ax.transAxes, fontsize=FONT_S)\n",
    "        \n",
    "        # Add the legend to the first subplot\n",
    "        if preds is not None:\n",
    "            handles, labels = axes[0].get_legend_handles_labels()\n",
    "            fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.9, 0.935), prop={'size': FONT_S-2})\n",
    "        \n",
    "        # Calculate MSE & MAE, and add it to the figure title.\n",
    "        if preds is not None:\n",
    "            preds_calc = preds[-pred_len:]\n",
    "            true_calc = true[-pred_len:]\n",
    "            mse = np.mean((preds_calc - true_calc) ** 2)\n",
    "            mae = np.mean(np.abs(preds_calc - true_calc))\n",
    "            fig.suptitle(f'MSE: {mse:.3f}, MAE: {mae:.3f}', fontsize=FONT_L, y=0.915)\n",
    "        \n",
    "        plt.savefig(name, bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # y[0].shape: [context_len, n_vars]\n",
    "    # y_pred[0].shape: [pred_len, n_vars]\n",
    "    cur_gt = np.concatenate([x[0].cpu().numpy(), y[0].cpu().numpy()], axis=0)\n",
    "    cur_pred = np.concatenate([x[0].cpu().numpy(), y_pred[0].cpu().numpy()], axis=0)\n",
    "\n",
    "    visual_ts(true=cur_gt, preds=cur_pred, \n",
    "              name=FIGURES_DIR+'prediction.pdf', lookback_len_visual=x.shape[1], pred_len=y.shape[1])\n",
    "    \n",
    "\n",
    "    # Save the figures\n",
    "    def save_image(image, title='', cur_nvars=1, cur_color_list=None, save_path='image.pdf'):\n",
    "        # image is [H, W, 3]\n",
    "        \n",
    "        # handle the color channels of the image according to the color_list\n",
    "        cur_image = torch.zeros_like(image)\n",
    "        cur_image = cur_image.cpu()\n",
    "        \n",
    "        height_per_var = image.shape[0] // cur_nvars\n",
    "        for i in range(cur_nvars):\n",
    "            cur_color = cur_color_list[i]\n",
    "            cur_image[i*height_per_var:(i+1)*height_per_var, :, cur_color] = \\\n",
    "                (image[i*height_per_var:(i+1)*height_per_var, :, cur_color].cpu() * imagenet_std[cur_color] + imagenet_mean[cur_color]) * 255\n",
    "        cur_image = torch.clip(cur_image, 0, 255).int()\n",
    "        \n",
    "        plt.plot(figsize=(10, 10))\n",
    "        plt.imshow(cur_image)\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        # plt.show()\n",
    "    \n",
    "    save_image(input_image[0, 0], 'input', nvars, color_list, save_path=FIGURES_DIR+'input.pdf')\n",
    "    save_image(reconstructed_image[0, 0], 'reconstructed', nvars, color_list, save_path=FIGURES_DIR+'reconstructed.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Forecasting\n",
    "\n",
    "- Example 1: a multivariate time series dataset ETTm1. \n",
    "    - Since ETTm1 is not included in the pre-trained datasets of VisionTS++, this is a zero-shot forecasting task.\n",
    "\n",
    "ATTENTION: If you have problems downloading the ETTm1 dataset through the following code, you can also download it from [Google Drive](https://drive.google.com/drive/folders/13Cg1KYOlzM5C7K8gK8NfC-F3EYxkM3D2?usp=sharing) or [Baidu Netdisk](https://pan.baidu.com/s/1r3KhGd0Q9PJIUZdfEYoymg?pwd=i9iy), from [Time-Series-Library](https://github.com/thuml/Time-Series-Library?tab=readme-ov-file).\n",
    "\n",
    "In this demo, we use the last window of ETTm1 dataset for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ===== Load ETTm1 data =====\n",
    "if not os.path.exists(\"./datasets\"):\n",
    "    os.makedirs(\"./datasets\")\n",
    "\n",
    "# The path of the ETTm1 data file\n",
    "data_path = \"./datasets/ETTm1.csv\"\n",
    "\n",
    "# ATTENTION: If you have problems downloading the ETTm1 dataset through the following code, \n",
    "# you can also download it from [Google Drive](https://drive.google.com/drive/folders/13Cg1KYOlzM5C7K8gK8NfC-F3EYxkM3D2?usp=sharing) \n",
    "# or [Baidu Netdisk](https://pan.baidu.com/s/1r3KhGd0Q9PJIUZdfEYoymg?pwd=i9iy).\n",
    "# The data link is from [Time-Series-Library](https://github.com/thuml/Time-Series-Library?tab=readme-ov-file).\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading ETTm1.csv...\")\n",
    "    \n",
    "    url = \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm1.csv\"\n",
    "    df = pd.read_csv(url)\n",
    "    df.to_csv(data_path, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "# transform the date column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "print(f\"{df.shape = }\")\n",
    "\n",
    "cols = df.columns[:]\n",
    "data = df[cols].values  # shape: [T, num_vars]\n",
    "\n",
    "print(f\"Loaded ETTm1 data with shape: {data.shape}\")\n",
    "\n",
    "\n",
    "# ===== Normalize the input data =====\n",
    "training_len = int(len(data) * 0.7)  # 70% of data for training\n",
    "x_training = data[:training_len, :]\n",
    "x_mean = x_training.mean(axis=0, keepdims=True)\n",
    "x_std = x_training.std(axis=0, keepdims=True)\n",
    "print(f\"x_mean.shape = {x_mean.shape}, x_std.shape = {x_std.shape}\")\n",
    "\n",
    "data_normalized = (data - x_mean) / x_std\n",
    "print(f\"data_normalized shape = {data_normalized.shape}\")\n",
    "\n",
    "\n",
    "# ===== Create inference windows =====\n",
    "def create_inference_windows(data, in_len=96, out_len=24):\n",
    "    \"\"\"\n",
    "    Generate x (input) and y (output) for inference.\n",
    "    Only the last window is used for inference.\n",
    "    \"\"\"\n",
    "    T = len(data)\n",
    "    assert T >= in_len + out_len, \"Too short data for inference\"\n",
    "\n",
    "    # we randomly select the last window for inference\n",
    "    end_idx = T\n",
    "    start_idx = end_idx - (in_len + out_len)\n",
    "\n",
    "    x = data[start_idx:start_idx + in_len]  # input segment\n",
    "    y = data[start_idx + in_len:end_idx]    # output segment\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# set the input and output length\n",
    "in_len = 960\n",
    "out_len = 394\n",
    "\n",
    "x, y = create_inference_windows(data_normalized, in_len=in_len, out_len=out_len)\n",
    "\n",
    "print(f\"x.shape = {x.shape}, y.shape = {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of predictions\n",
    "\n",
    "There will be two figures generated from this cell. \n",
    "\n",
    "The first figure includes the transformeed image, the reconstruction image; the original time series (first variate and last window of ETTm1 datasetst), and the predicted time series of quantile level of 10\\%, 20\\%, ..., 90\\%, which flexibly approximates an output distribution for probabilistic forecasting. \n",
    "\n",
    "The second figure shows the median prediction (50\\%) of 7 variates of ETTm1 dataset, showing VisionTS++'s capability of handling multivariate time series forecasting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since ETTm1 is sampled every 15 minutes, so a possible choice of its periodicity is 96.\n",
    "periodicity_list = freq_to_seasonality_list(\"15Min\")\n",
    "periodicity = periodicity_list[0]\n",
    "print(periodicity)\n",
    "# expected: periodicity = 96\n",
    "\n",
    "nvars = x.shape[1]  # 7 variables for ETTm1\n",
    "\n",
    "# You canr randomly assign colors to each variables\n",
    "# Our recommendation is to follow a RGB iteration form (R, G, B, R, G, B,...) in order for simplicity.\n",
    "color_list = [i % 3 for i in range(nvars)]\n",
    "\n",
    "\n",
    "# run() function accepts raw data\n",
    "run(x, y, periodicity=periodicity, nvars=nvars, color_list=color_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization on more complicated examples can be found in Appendix C of our paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visiontspp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
